---
title: "SAMHSA NSSATS modeling and formal testing "
output: html_notebook
---


Checking assumptions of linear regression: normal distribution and continuous variable
```{r}
#all of the forprofit distributions are skewed right 
hist(state_final_df[state_final_df$profit_type=="Forprofit", ]$count_tx, 
     xlab = "Number of Treatment Centers", 
     main = "For-Profit Treatment Center Distribution")

#add the mean and variance onto the plot
text(600,150,paste("mean = ", round(mean(state_final_df[state_final_df$profit_type=="Forprofit", ]$count_tx), 2), "\n variance = ", round(var(state_final_df[state_final_df$profit_type=="Forprofit", ]$count_tx),2)))

#all of the nonprofit distributions are skewed right
hist(state_final_df[state_final_df$profit_type=="Nonprofit", ]$count_tx, xlab = "Number of Treatment Centers", main = "Non-Profit Treatment Center Distribution")
#add the mean and variance onto the plot
text(600,120,paste("mean = ", round(mean(state_final_df[state_final_df$profit_type=="Nonprofit", ]$count_tx), 2), "\n variance = ", round(var(state_final_df[state_final_df$profit_type=="Nonprofit", ]$count_tx),2)))

#all of the government distributions are skewed right
hist(state_final_df[state_final_df$profit_type=="Government", ]$count_tx, xlab = "Number of Treatment Centers", main = "Government Treatment Center Distribution")
#add the mean and variance onto the plot
text(150,120,paste("mean = ", round(mean(state_final_df[state_final_df$profit_type=="Government", ]$count_tx, na.rm = TRUE), 2), "\n variance = ", round(var(state_final_df[state_final_df$profit_type=="Government", ]$count_tx, na.rm = TRUE),2)))


```


For count data, we have options of poisson or negative binomial distributions. 

Negative binomial distribution is appropriate if there is sparse data (not applicable) or overdispersion (variance > mean)
```{r}
print("For Profit Overdispersion")
var(profit_states_1419$count_Forprofit_2015) > mean(profit_states_1419$count_Forprofit_2015)
var(profit_states_1419$count_Forprofit_2016) > mean(profit_states_1419$count_Forprofit_2016)
var(profit_states_1419$count_Forprofit_2017) > mean(profit_states_1419$count_Forprofit_2017)
var(profit_states_1419$count_Forprofit_2018) > mean(profit_states_1419$count_Forprofit_2018)
var(profit_states_1419$count_Forprofit_2019) > mean(profit_states_1419$count_Forprofit_2019)

print("Non Profit Overdispersion")
var(profit_states_1419$count_Nonprofit_2015) > mean(profit_states_1419$count_Nonprofit_2015)
var(profit_states_1419$count_Nonprofit_2016) > mean(profit_states_1419$count_Nonprofit_2016)
var(profit_states_1419$count_Nonprofit_2017) > mean(profit_states_1419$count_Nonprofit_2017)
var(profit_states_1419$count_Nonprofit_2018) > mean(profit_states_1419$count_Nonprofit_2018)
var(profit_states_1419$count_Nonprofit_2019) > mean(profit_states_1419$count_Nonprofit_2019)

print("Government Overdispersion")
var(profit_states_1419$count_Government_2015) > mean(profit_states_1419$count_Government_2015)
var(profit_states_1419$count_Government_2016) > mean(profit_states_1419$count_Government_2016)
var(profit_states_1419$count_Government_2017, na.rm = TRUE) > mean(profit_states_1419$count_Government_2017, na.rm = TRUE) #there is 1 null value in this data 
var(profit_states_1419$count_Government_2018) > mean(profit_states_1419$count_Government_2018)
var(profit_states_1419$count_Government_2019) > mean(profit_states_1419$count_Government_2019)


```




split the data into a forprofit, nonprofit, and government dataset 


```{r LRT for best demand variables }
library(lme4)
#code to determine the best subset of demand variables to include 
demandvars_lrt <- function(df, demandvars_list, verbose = FALSE){
  
  model_list <- list()
  
  #loop through to peel off the last element from demandvars_list at a time, until only the first demand variable is left 
  for(i in 0:(length(demandvars_list)-1))
  {
    demand_variables_touse <- demandvars_list[1:(length(demandvars_list) - i)]
    
    #create the list of demand variables to use 
    demand_vars <- paste(demand_variables_touse, collapse = " + ")
  
    control_vars <- " + medicaid_enroll_percapita + 
             percapita_health_spending_thousands + 
             pop_millions +
             (1|State_Name) + (1 | year )"
    
    #paste the formula together 
    nb_formula <- formula(paste("count_tx ~", demand_vars, control_vars))
    
    #build the model and save in the list
    model <- glmer.nb(nb_formula, data = df, 
                      control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)))
    
    #append the model to the list 
    model_list[[i + 1]] <- model

  }
  
  #store the maximum likelihood model - start by assuming it is the full model 
  max_model <- model_list[[1]]
  
  #compare the models pairwise using the LRT 
  for (i in seq(1:(length(model_list) - 1) ))
  {
      anova_results <- anova(max_model, model_list[[i+1]])
      
      if (verbose)
      {
        print("________________RESULTS_________________")
        print(anova_results)
      }
      
      #get the p-value
      pval <- anova_results$'Pr(>Chisq)'[2]
      
      if (pval < 0.05)
      {
        print(paste("The more complex model improves fit sufficiently, p = ", pval))
      }
      else
      {
        print(paste("The more complex model does not improve fit sufficiently. considering the less complex model to be better", p = pval))
        max_model <- model_list[[i+1]]
      }
      
      print("Best model so far includes: ")
      print(all.vars(formula(max_model)))
  }
}

```


```{r LRT for best control variables, given best demand variables}
#code to determine the best subset of demand variables to include 
controlvars_lrt <- function(df, bestdemandvars_str, controlvars_list, verbose = FALSE){
  
  model_list <- list()
  
  #loop through to peel off the last element from controlvars_list at a time, until only the first control variable is left 
  for(i in 0:(length(controlvars_list) - 1 ))
  {
    control_variables_touse <- controlvars_list[1:(length(controlvars_list) - i)]
    
    #create the list of control variables to use 
    control_vars <- paste(control_variables_touse, collapse = " + ")
  
    random_effects <- " +
             (1|State_Name) + (1 | year) "
    
    #paste the formula together 
    nb_formula <- formula(paste("count_tx ~", bestdemandvars_str, control_vars, random_effects))

    #build the model and save in the list
    model <- glmer.nb(nb_formula, data = df, 
                      control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)))
    
    #append the model to the list 
    model_list[[i + 1]] <- model

  }
  

  #store the maximum likelihood model - start by assuming it is the full model 
  max_model <- model_list[[1]]
  
  #compare the models pairwise using the LRT 
  for (i in seq(1:(length(model_list) - 1) ))
  {
      anova_results <- anova(max_model, model_list[[i+1]])
      
      #get the p-value
      pval <- anova_results$'Pr(>Chisq)'[2]
      
      if (verbose)
      {
        print("________________RESULTS_________________")
        print(anova_results)
      }
      
      if (pval < 0.05)
      {
        print(paste("The more complex model improves fit sufficiently, p = ", pval))
      }
      else
      {
        print(paste("The more complex model does not improve fit sufficiently. considering the less complex model to be better", p = pval))
        max_model <- model_list[[i+1]]
      }
      
      print("Best model so far includes: ")
      print(all.vars(formula(max_model)))
  }
}
```




After controlling for random effects of both year and state, these results are not significant. I am wondering whether the date range is limiting us - i want to try using the following variables to measure demand, which have been available for a longer period of time 

These are available starting at 2013-2014: 
alcohol use disorder in the past year, 
cocaine use in the past year, 
heroin use in the past year, 
marijuana use in the past month 
(probably won't include tobacco use because it isnt as often treated in a specialty facility)

```{r}
state_forprofitUSE <- state_final_df %>% 
                    arrange(State_Abbrev, year) %>%  # Ensure data is ordered by State_Abbrev and year
                    group_by(State_Abbrev) %>%
                    filter(profit_type == "Forprofit", !is.na(prev2years_drugusecalc)) #2014 does not have this data

state_nonprofitUSE <- state_final_df %>% 
                    arrange(State_Abbrev, year) %>%  # Ensure data is ordered by State_Abbrev and year
                    group_by(State_Abbrev) %>%
                    filter(profit_type == "Nonprofit", !is.na(prev2years_drugusecalc))

state_govtUSE <- state_final_df %>% 
                    arrange(State_Abbrev, year) %>%  # Ensure data is ordered by State_Abbrev and year
                    group_by(State_Abbrev) %>%
                    filter(profit_type == "Government", !is.na(prev2years_drugusecalc))

```

```{r}
library(corrplot)
corr_matrix <- cor(state_forprofitUSE[, c("pop_millions", "percapita_health_spending_thousands", "medicaid_enroll_percapita", "prev2years_drugusecalc", "prev2years_aud", "prev2years_marijuana")])

write.csv(corr_matrix, "correlation_matrix.csv")
corrplot(corr_matrix, method="circle")
```



```{r}
demandvars_lrt(state_forprofitUSE, c("prev2years_drugusecalc", "prev2years_aud", "prev2years_marijuana"), verbose = TRUE)
#demandvars_lrt(state_forprofitUSE, c("prev2years_cocaine", "prev2years_heroin",  "prev2years_aud", "prev2years_marijuana"))
```
```{r}
#controlvars_lrt(state_forprofitUSE, "prev2years_cocaine + prev2years_heroin + prev2years_aud + ", c("pop_millions", "medicaid_enroll_percapita", "percapita_health_spending_thousands"))
controlvars_lrt(state_forprofitUSE, "prev2years_drugusecalc + prev2years_aud + ", c("pop_millions",  "percapita_health_spending_thousands", "medicaid_enroll_percapita"), verbose = TRUE)
```


relationship is not significant when adding cocaine and heroin separately
```{r}
glmer_forprofitUSE <- glmer.nb(count_tx ~ #prev2years_cocaine +  prev2years_heroin +  prev2years_aud + 
                                 prev2years_drugusecalc + prev2years_aud + #prev2years_marijuana +
                                pop_millions + #percapita_health_spending_thousands + medicaid_enroll_percapita + 
                                (1|State_Name)+ (1 | year),
                              control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)),
                              data = state_forprofitUSE)

summary(glmer_forprofitUSE)

```

```{r}
demandvars_lrt(state_nonprofitUSE, c("prev2years_drugusecalc", "prev2years_aud", "prev2years_marijuana"), verbose = TRUE)
```


```{r}
controlvars_lrt(state_nonprofitUSE, "prev2years_drugusecalc + prev2years_aud + ", c("pop_millions", "medicaid_enroll_percapita", "percapita_health_spending_thousands"))
```


```{r}
glmer_nonprofitUSE <- glmer.nb(count_tx ~ prev2years_drugusecalc + prev2years_aud + #prev2years_marijuana +
                                pop_millions + #percapita_health_spending_thousands + medicaid_enroll_percapita + 
                                (1|State_Name)+ (1 | year),
                              control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)),
                              data = state_nonprofitUSE)

summary(glmer_nonprofitUSE)
```
Because the above threw convergence errors, running allFit
```{r}
summary(allFit(glmer_nonprofitUSE))
```


test the best model for government
```{r}
demandvars_lrt(state_govtUSE, c("prev2years_drugusecalc", "prev2years_aud", "prev2years_marijuana"))
```
```{r}
controlvars_lrt(state_govtUSE, "prev2years_drugusecalc + prev2years_aud + ", c("pop_millions", "medicaid_enroll_percapita", "percapita_health_spending_thousands"))
```



```{r}


glmer_govt <- glmer.nb(count_tx ~ prev2years_drugusecalc + prev2years_aud + #prev2years_marijuana +
                                pop_millions + #percapita_health_spending_thousands + medicaid_enroll_percapita + 
                                (1|State_Name) + (1 | year),
                              control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)),
                              data = state_govtUSE)

summary(glmer_govt)
```

```{r}
summary(allFit(glmer_govt))

```

Extract the state and year random effects
```{r}
state_randomeffects <- ranef(glmer_forprofitUSE)$State_Name
#relabel the dataframe
state_randomeffects <- data.frame("State_Name" =row.names(state_randomeffects), "Intercept" = state_randomeffects[,"(Intercept)"])


year_randomeffects <- ranef(glmer_forprofitUSE)$year
#relabel the dataframe
year_randomeffects <- data.frame("year" =row.names(year_randomeffects), "Intercept" = year_randomeffects[,"(Intercept)"])

```

Plot
```{r}
library(ggplot2)
library(maps)
library(mapdata)
library(plotly)
library(sf)

usa <- map_data('usa')
state <- map_data("state")

state_randomeffects %>%
  mutate(state_lower = tolower(State_Name)) %>% 
  left_join(state, by = c("state_lower"="region")) %>%
 ggplot(aes(x=long, y=lat, fill= Intercept, group=group)) + 
                      scale_fill_gradient2(midpoint = 0, low = "red", mid = "grey", high = "green") +
                       #scale_fill_gradient(low = "darkgrey", high = "blue", limits = c(-1.75, 1.3)) +
                       geom_polygon(color = "white") + 
                      ggtitle('Random Effects per State in For-Profit Model') + 
                       coord_fixed(1.3)

# ranef(glmer_forprofitUSE)$year

```




































































Analysis using only the post-2016 measures: I think this was too limited by sample size 

The best set of demand variables for the for profit model is only using drug use

not including the tx metric because it is so highly correlated with SUD. I am choosing to include the SUD metric because i conceptually expect it to be most closely correlated with SUD treatment, and because the "needing but not receiving treatment" measure is complex (i.e. does that indicate more demand, or just an existing gap?) whereas drug use and binge drinking might not directly translate to wanting or needing treatment 
```{r}
demandvars_lrt(state_forprofit, c("prev2years_sud", "prev2years_druguse", "prev2years_bingedrinking")) #"prev2years_needingtx"
print("___________________________________________")
#demandvars_lrt(state_forprofit, c("needing_tx", "illicit_druguse", "binge_drinking"))

```

Keeping all three of the control variables (population, medicaid enrollment, and percapita health spending) improves the fit 
```{r}
controlvars_lrt(state_forprofit, "prev2years_sud +", c("pop_millions", "medicaid_enroll_percapita", "percapita_health_spending_thousands"))
```


Now that we have chosen the model with the best demand variables and the best control variables according to the likelihood ratio test, we can review the coefficients on that model. 

There is a negative, nearly significant relationship between the previous 2 years' prevalence of SUD and the count of treatment centers in a given state-year. 

State population and percapita health spending is associated with a significant increase in the number of for-profit treatment centers. 

```{r}
glmer_forprofitFINAL <- glmer.nb(count_tx ~ prev2years_sud + 
                                   pop_millions + percapita_health_spending_thousands + medicaid_enroll_percapita + 
                                   (1 | State_Name) + (1 | year),
                                 data = state_forprofit)

summary(glmer_forprofitFINAL)
```





Now do the same thing for non-profit, starting by identifying the best demand variables 

```{r}
demandvars_lrt(state_nonprofit, c("prev2years_sud", "prev2years_druguse", "prev2years_bingedrinking")) #prev2years_sud
print("___________________________________________")
#demandvars_lrt(state_nonprofit, c("needing_tx", "illicit_druguse", "binge_drinking"))

```

Keeping all the control variables improves the fit.
```{r}
controlvars_lrt(state_nonprofit, "prev2years_sud +", c("pop_millions", "medicaid_enroll_percapita", "percapita_health_spending_thousands"))
```



The tests  suggest that the most simple model is best. But because of the iteration warning, I will run the AllFit() call to ensure that all model approaches converge to the same result (as recommended by ?covergence ). If all approaches converge to near identical estimates, I will assume this is a false positive convergence warning.

```{r}
#?convergence

glmer_nonprofitFINAL <- glmer.nb(count_tx ~ prev2years_sud + prev2years_druguse +
                                pop_millions + percapita_health_spending_thousands + medicaid_enroll_percapita + 
                                (1|State_Name) + (1 | year),
                                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)),
                              data = state_nonprofit)
glmer_nonprofitALL <- allFit(glmer_nonprofitFINAL)

ss_nonprofitALL <- summary(glmer_nonprofitALL)
ss_nonprofitALL$fixef #all of these fixed effects are very similar, so i think it is safe to proceed with the model results 
```


Now that we have chosen the best model and investigated the convergence warning, we can look at the coefficients. 

There is no significant relationship between the previous 2 years' amount of people with a SUD and the count of treatment centers in a given state-year. 

State population and percapita health spending is associated with a significant increase in the number of non-profit treatment centers. Medicaid enrollment per-capita is associated with a significant decrease in the number of non-profit treatment centers. 
```{r}
summary(glmer_nonprofitFINAL)
```



Finally, we can follow the same steps for the Government-run treatment centers
```{r}
demandvars_lrt(state_govt, c("prev2years_sud", "prev2years_druguse", "prev2years_bingedrinking"))

```

The model with only the needing but not receiving treatment metric is best. Now we can evaluate the best control variables. Here we see that the best model does not include the control variables... but we should probably use them for comparability to the other models 
```{r}
controlvars_lrt(state_govt, "prev2years_sud +", c("pop_millions", "medicaid_enroll_percapita", "percapita_health_spending_thousands"))
```


```{r}
glmer_govtFINAL <- glmer.nb(count_tx ~ prev2years_sud + 
                                pop_millions + percapita_health_spending_thousands + medicaid_enroll_percapita + 
                                (1|State_Name) + (1 | year),
                              control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e6)),
                              data = state_govt)

glmer_govtALL <- allFit(glmer_govtFINAL)
ss_govtALL <- summary(glmer_govtALL)
ss_govtALL$fixef #these are all similar so considering it to be accurate

```
```{r}
summary(glmer_govtFINAL)
```